# Building an image  that includes the summarizer and that can use
# a GPU. When run a container use the "--gpus all" flag.


FROM nvidia/cuda:12.5.0-base-ubuntu22.04


# For the final production image we don't need tree and emacs.
# Ollama uses lshw (list hardware) in order to find the GPU.
# The base image comes with Python 3.10, but the pip module is not installed.

RUN apt-get -y update \
	&& apt-get install -y git curl systemctl tree emacs \
	&& apt-get install -y nvidia-container-toolkit lshw \
	&& apt-get install -y python3-pip

RUN curl -fsSL https://ollama.com/install.sh | sh


# The setup.sh script is responsible for loading the llama3 model.

COPY setup.sh ./
RUN sh setup.sh


WORKDIR /app
COPY requirements.txt ./
RUN pip install -r requirements.txt


# Just cloning the repositories, the run.sh script will pull updates as needed

RUN git clone https://github.com/lapps-xdd/xdd-docstructure code/xdd-docstructure \
	&& git clone https://github.com/lapps-xdd/xdd-processing code/xdd-processing \
	&& git clone https://github.com/lapps-xdd/xdd-LLMs code/xdd-llms \
	&& git clone https://github.com/lapps-xdd/xdd-terms code/xdd-terms


# Copy this last since it is most likely to change

COPY run.sh . 
